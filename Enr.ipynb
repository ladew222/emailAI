{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science with Python\n",
    "\n",
    "Data Science involves extracting insights from structured and unstructured data. Both Python and R are prominent languages in this domain, each with its strengths.\n",
    "\n",
    "## Python in Data Science\n",
    "\n",
    "Python boasts a wide array of libraries and frameworks that facilitate data science tasks. Some of the most popular include:\n",
    "- **Pandas**: For data manipulation and analysis.\n",
    "- **NumPy**: Supports numerical operations with arrays and matrices.\n",
    "- **Scikit-learn**: Comprehensive library for machine learning.\n",
    "- **Matplotlib** and **Seaborn**: For data visualization.\n",
    "\n",
    "Python's versatility extends beyond just data science. It's used in web development, automation, and more. This broad applicability often makes it a favorite for those who want a general-purpose language with data science capabilities.\n",
    "\n",
    "### Advantages of Python:\n",
    "- **Versatility**: From web apps to machine learning, Python is versatile.\n",
    "- **Community**: A large and growing community means more libraries, tools, and resources.\n",
    "- **Integration**: Python integrates easily with other languages and platforms.\n",
    "\n",
    "## R in Data Science\n",
    "\n",
    "R was designed specifically for data analysis and visualization. It's a favorite among statisticians and data miners.\n",
    "\n",
    "Key packages in R for data science include:\n",
    "- **dplyr**: For data manipulation.\n",
    "- **ggplot2**: A powerful visualization package.\n",
    "- **caret**: Streamlines the model training process for machine learning.\n",
    "- **tidyr**: Helps in tidying data.\n",
    "\n",
    "### Advantages of R:\n",
    "- **Richness for Stats**: Given its origins, R excels in statistical modeling and testing.\n",
    "- **Visualization**: Libraries like ggplot2 provide advanced visualization capabilities.\n",
    "- **Comprehensive**: R has a package for almost any statistical analysis imaginable.\n",
    "\n",
    "## Python vs. R: When to Use Which?\n",
    "\n",
    "1. **For General-Purpose & Web Apps**: Python's diverse libraries and frameworks make it suitable for a broader range of applications.\n",
    "2. **Deep Learning**: Python takes the lead due to frameworks like TensorFlow and PyTorch.\n",
    "3. **Statistical Analysis & Research**: R remains a favorite among researchers and statisticians for its in-depth statistical packages.\n",
    "4. **Learning Curve**: Python is often regarded as having a gentler learning curve for beginners due to its readable syntax.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Both Python and R have their places in the world of data science. While Python offers versatility and a wide range of applications, R remains deep-rooted in statistical analysis and research. Choosing between them often depends on the specific task at hand and personal or organizational preference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "data.rename(columns={'old_name': 'new_name'}, inplace=True)\n",
    "\n",
    "# Handling missing values\n",
    "data.dropna(subset=['column_name'], inplace=True)  # Removing rows with missing values\n",
    "data['column_name'].fillna(value='DEFAULT', inplace=True)  # Filling missing values\n",
    "\n",
    "# Applying a function to a column\n",
    "data['new_column'] = data['column_name'].apply(lambda x: x*2)\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "data = pd.get_dummies(data, columns=['categorical_column'])\n",
    "\n",
    "\n",
    "# Summary statistics for numeric columns\n",
    "data.describe()\n",
    "\n",
    "# Data types of columns\n",
    "data.dtypes\n",
    "\n",
    "# Count of unique values in a column\n",
    "data['column_name'].nunique()\n",
    "\n",
    "# Frequency of unique values\n",
    "data['column_name'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Introduction to Bag of Words & The Enron Dataset\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "In the realm of text data representation, various models exist to transform human-readable text into machine-understandable formats. One of the simplest, yet most effective of these methods, is the Bag of Words (BoW) model. In this document, we will explore the BoW model and its application on the Enron dataset.\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#### **What is BoW?**\n",
    "\n",
    "Bag of Words (BoW) is a method to represent text data. Instead of viewing a document in its original sequential format, BoW views it as a collection (or \"bag\") of its individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Hello': 1, 'world!': 1, 'This': 1, 'is': 1, 'a': 1, 'simple': 1, 'BoW': 1, 'example.': 1})\n"
     ]
    }
   ],
   "source": [
    "# Python block: Simple BoW Representation\n",
    "from collections import Counter\n",
    "\n",
    "def bow_representation(text):\n",
    "    return Counter(text.split())\n",
    "\n",
    "example_text = \"Hello world! This is a simple BoW example.\"\n",
    "print(bow_representation(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "While BoW is straightforward, it does have its drawbacks. For instance, it ignores the order of words, potentially losing context. However, for many tasks, it's more than sufficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Significance of the Dataset**\n",
    "The Enron dataset is not just a collection of emails. It's a snapshot of corporate America, capturing business practices, decision-making processes, and even personal communications. It has been used for various research purposes, especially in the field of Natural Language Processing (NLP) and machine learning, to develop and test algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Use BoW?**\n",
    "Given its simplicity, BoW is computationally efficient. It's a great starting point for text classification tasks and offers a foundational understanding before delving into more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goodbye' 'hello' 'world']\n",
      "[[0 1 1]\n",
      " [1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Python block: BoW with sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([\"Hello world\", \"Goodbye world\"])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "While BoW is straightforward, it does have its drawbacks. For instance, it ignores the order of words, potentially losing context. However, for many tasks, it's more than sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Hello': 1, 'world!': 1, 'This': 1, 'is': 1, 'a': 1, 'simple': 1, 'BoW': 1, 'example.': 1})\n"
     ]
    }
   ],
   "source": [
    "# Python block: Simple BoW Representation\n",
    "from collections import Counter\n",
    "\n",
    "def bow_representation(text):\n",
    "    return Counter(text.split())\n",
    "\n",
    "example_text = \"Hello world! This is a simple BoW example.\"\n",
    "print(bow_representation(example_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goodbye' 'hello' 'world']\n",
      "[[0 1 1]\n",
      " [1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Python block: BoW with sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([\"Hello world\", \"Goodbye world\"])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>&gt;&gt;&gt; [1]Contact Me Now to Make $100 Today!$LINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Act now to keep your life on the go!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Choose between $500 and $10000 dollars with up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>Click above to earn today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>Click here to receive your first $10 today:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                              Email\n",
       "0    No     >>> [1]Contact Me Now to Make $100 Today!$LINK\n",
       "1    No               Act now to keep your life on the go!\n",
       "2    No  Choose between $500 and $10000 dollars with up...\n",
       "3    No                         Click above to earn today.\n",
       "4    No        Click here to receive your first $10 today:"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Ask0729-fixed.txt\", delimiter=\"\\t\", header=None, names=['Label', 'Email'])\n",
    "\n",
    "# Display the first few rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3657</td>\n",
       "      <td>3657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>3657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>No</td>\n",
       "      <td>&gt;&gt;&gt; [1]Contact Me Now to Make $100 Today!$LINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                                           Email\n",
       "count   3657                                            3657\n",
       "unique     2                                            3657\n",
       "top       No  >>> [1]Contact Me Now to Make $100 Today!$LINK\n",
       "freq    1938                                               1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3657, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Check the shape of the dataset after dropping missing values\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [>, >, >, [, 1, ], Contact, Me, Now, to, Make,...\n",
       "1     [Act, now, to, keep, your, life, on, the, go, !]\n",
       "2    [Choose, between, $, 500, and, $, 10000, dolla...\n",
       "3                   [Click, above, to, earn, today, .]\n",
       "4    [Click, here, to, receive, your, first, $, 10,...\n",
       "Name: Tokenized_Email, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenizing the 'Email' column\n",
    "data['Tokenized_Email'] = data['Email'].apply(word_tokenize)\n",
    "data['Tokenized_Email'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Natural language processing often requires reducing words to a more generic form for analysis, modeling, or comparison. Two common techniques used for this purpose are stemming and lemmatization.\n",
    "\n",
    "Stemming reduces words to their root form, while lemmatization reduces them to their base or dictionary form\n",
    "\n",
    "Advantages of Stemming:\n",
    "\n",
    "Reduces the corpus of words the model is exposed to and explicitly correlates words with similar meanings.\n",
    "Typically faster as it simply chops off endings (and sometimes beginnings) without understanding the context.\n",
    "Limitations of Stemming:\n",
    "\n",
    "It can produce words that are not actual words. For instance, \"flies\" becomes \"fli\".\n",
    "It may not always be semantically correct. For instance, \"university\" and \"universe\" might be stemmed to the same root of \"univers\".\n",
    "\n",
    "\n",
    "running\" → \"run\"\n",
    "\"flies\" → \"fli\"\n",
    "\"happily\" → \"happili\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Email</th>\n",
       "      <th>Stemmed_Email</th>\n",
       "      <th>Lemmatized_Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[&gt;, &gt;, &gt;, [, 1, ], Contact, Me, Now, to, Make,...</td>\n",
       "      <td>[&gt;, &gt;, &gt;, [, 1, ], contact, me, now, to, make,...</td>\n",
       "      <td>[&gt;, &gt;, &gt;, [, 1, ], Contact, Me, Now, to, Make,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Act, now, to, keep, your, life, on, the, go, !]</td>\n",
       "      <td>[act, now, to, keep, your, life, on, the, go, !]</td>\n",
       "      <td>[Act, now, to, keep, your, life, on, the, go, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Choose, between, $, 500, and, $, 10000, dolla...</td>\n",
       "      <td>[choos, between, $, 500, and, $, 10000, dollar...</td>\n",
       "      <td>[Choose, between, $, 500, and, $, 10000, dolla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Click, above, to, earn, today, .]</td>\n",
       "      <td>[click, abov, to, earn, today, .]</td>\n",
       "      <td>[Click, above, to, earn, today, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Click, here, to, receive, your, first, $, 10,...</td>\n",
       "      <td>[click, here, to, receiv, your, first, $, 10, ...</td>\n",
       "      <td>[Click, here, to, receive, your, first, $, 10,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Tokenized_Email   \n",
       "0  [>, >, >, [, 1, ], Contact, Me, Now, to, Make,...  \\\n",
       "1   [Act, now, to, keep, your, life, on, the, go, !]   \n",
       "2  [Choose, between, $, 500, and, $, 10000, dolla...   \n",
       "3                 [Click, above, to, earn, today, .]   \n",
       "4  [Click, here, to, receive, your, first, $, 10,...   \n",
       "\n",
       "                                       Stemmed_Email   \n",
       "0  [>, >, >, [, 1, ], contact, me, now, to, make,...  \\\n",
       "1   [act, now, to, keep, your, life, on, the, go, !]   \n",
       "2  [choos, between, $, 500, and, $, 10000, dollar...   \n",
       "3                  [click, abov, to, earn, today, .]   \n",
       "4  [click, here, to, receiv, your, first, $, 10, ...   \n",
       "\n",
       "                                    Lemmatized_Email  \n",
       "0  [>, >, >, [, 1, ], Contact, Me, Now, to, Make,...  \n",
       "1   [Act, now, to, keep, your, life, on, the, go, !]  \n",
       "2  [Choose, between, $, 500, and, $, 10000, dolla...  \n",
       "3                 [Click, above, to, earn, today, .]  \n",
       "4  [Click, here, to, receive, your, first, $, 10,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['Stemmed_Email'] = data['Tokenized_Email'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "data['Lemmatized_Email'] = data['Tokenized_Email'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "data[['Tokenized_Email', 'Stemmed_Email', 'Lemmatized_Email']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels\n",
    "\n",
    "In order to train a supervised learning model, we need labeled data. This means that each email in our dataset should have an associated label indicating whether it's 'personal' or 'professional'. If the dataset doesn't come pre-labeled, we might have to do this manually for a subset of emails. However, this can be a time-consuming process, so we'll skip this step for now and assume that the dataset is already labeled which it is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Email Length:**\n",
    "\n",
    "The length of an email might be indicative of its category. For instance, professional emails might be longer due to their formal nature. We also want the bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=5000)  # We limit to 5000 most frequent words\n",
    "X_bow = vectorizer.fit_transform(data['Email']).toarray()\n",
    "data['Email_Length'] = data['Email'].apply(len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Building the Bag of Words Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3657, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Display the shape to get an idea of the vectorized data\n",
    "print(X_bow.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Training/Testing Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Concatenate the BoW features and the Email_Length feature\n",
    "import numpy as np\n",
    "X_combined = np.hstack((X_bow, data['Email_Length'].values.reshape(-1, 1)))\n",
    "\n",
    "X_combined.shape\n",
    "\n",
    "# Split the data - 80% for training and 20% for testing, using our BoW representation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, data['Label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to text classification, several machine learning algorithms can be employed. For our task of email categorization based on Bag of Words features, a popular and effective choice is the Naive Bayes classifier, specifically the Multinomial Naive Bayes.\n",
    "\n",
    "Why Naive Bayes?\n",
    "\n",
    "Simplicity: Naive Bayes is based on the Bayes theorem with the 'naive' assumption of conditional independence between every pair of features. This simplicity makes it fast.\n",
    "\n",
    "Scalability: It scales well with the dataset's dimensionality, which is crucial given the high-dimensional nature of text data.\n",
    "\n",
    "Performance: Despite its simplicity, Naive Bayes tends to perform notably well on text classification tasks, especially when the dataset isn't enormous.\n",
    "\n",
    "Probabilistic Nature: Naive Bayes provides probabilities for each class, offering an understanding of the model's confidence in its predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# After training the model\n",
    "predicted_labels = clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model Evaluation\n",
    "\n",
    "In the process of building a classification model, once the model is trained, it's essential to evaluate its performance on unseen data. This will provide insights on how well the model might perform in a real-world setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Sensitivity: 0.84\n",
      "Specificity: 0.61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "sensitivity = recall_score(y_test, predicted_labels, pos_label='Yes')\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predicted_labels).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explination\n",
    "Definition: Accuracy is the ratio of correctly predicted instances to the total number of instances.\n",
    "Interpretation: In this context, an accuracy of 0.71 means that 71% of all the emails were correctly classified by the model. Put simply, for every 100 emails you run through the classifier, it will predict the correct label for 71 of them on average.\n",
    "2. Sensitivity (Recall) (0.84 or 84%):\n",
    "\n",
    "Definition: Sensitivity or Recall gives the true positive rate. It's the proportion of all the positive cases (in your case, let's say emails labeled 'Yes') that the model correctly identified as positive.\n",
    "Interpretation: A sensitivity of 0.84 indicates that the model correctly identified 84% of the actual positive cases. This means if there were 100 emails that were truly 'Yes', the model would detect 84 of them correctly while missing out on 16.\n",
    "3. Specificity (0.61 or 61%):\n",
    "\n",
    "Definition: Specificity is the true negative rate. It represents the proportion of all the negative cases (emails labeled 'No') that were correctly identified by the model.\n",
    "Interpretation: A specificity of 0.61 means that the model correctly identified 61% of the actual negative cases. So, if there were 100 emails that were truly 'No', the model would correctly classify 61 of them and incorrectly classify the remaining 39 as 'Yes'.\n",
    "\n",
    "The model is particularly good at identifying positive cases, with a high sensitivity of 84%. This implies it's effective at catching emails labeled 'Yes'.\n",
    "However, its specificity is a bit lower at 61%, which suggests that while it's good at identifying 'Yes' labels, it might misclassify a decent chunk of 'No' labeled emails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features\n",
    "\n",
    "The Bag of Words model represents text data in terms of word frequencies. However, often the sheer frequency of a word doesn't give its exact importance. That's where Term Frequency-Inverse Document Frequency (TF-IDF) comes in. It not only considers the frequency of a word in a particular document but also how frequent the word is across all documents.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Gives more weight to terms that are more specific to a particular document.\n",
    "Common words that are frequently present across all documents get lower weights, making it a better representation than simple counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's use a simple, hands-on example to illustrate the concept of TF-IDF.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose we have the following three documents:\n",
    "\n",
    "1. `Doc1`: \"I love blue skies.\"\n",
    "2. `Doc2`: \"Blue and bright skies are beautiful.\"\n",
    "3. `Doc3`: \"Love the blue color of the ocean.\"\n",
    "\n",
    "Let's compute the TF-IDF values for the word \"blue\" in each document:\n",
    "\n",
    "**1. Term Frequency (TF) of \"blue\"**:\n",
    "\n",
    "For `Doc1`: \n",
    "TF(\"blue\", Doc1) = (Number of times \"blue\" appears in Doc1) / (Total number of terms in Doc1)\n",
    "                 = 1/4\n",
    "                 = 0.25\n",
    "\n",
    "For `Doc2`: \n",
    "TF(\"blue\", Doc2) = 1/6 = 0.1667\n",
    "\n",
    "For `Doc3`: \n",
    "TF(\"blue\", Doc3) = 1/7 ≈ 0.1429\n",
    "\n",
    "**2. Inverse Document Frequency (IDF) of \"blue\"**:\n",
    "\n",
    "Since \"blue\" appears in all three documents:\n",
    "\n",
    "IDF(\"blue\") = log(Total number of documents / Number of documents with \"blue\")\n",
    "            = log(3/3)\n",
    "            = 0 (because log(1) = 0)\n",
    "\n",
    "**3. Compute TF-IDF**:\n",
    "\n",
    "For `Doc1`: \n",
    "TF-IDF(\"blue\", Doc1) = TF(\"blue\", Doc1) * IDF(\"blue\")\n",
    "                    = 0.25 * 0\n",
    "                    = 0\n",
    "\n",
    "Similarly, TF-IDF values for \"blue\" in `Doc2` and `Doc3` will also be 0 due to the IDF value being 0.\n",
    "\n",
    "From this example, since the term \"blue\" appears in all documents, it's considered not to be a distinctive word, and hence, its TF-IDF value is 0.\n",
    "\n",
    "### Another Example: Word \"ocean\" in `Doc3`\n",
    "\n",
    "**1. Term Frequency (TF) of \"ocean\"**:\n",
    "\n",
    "TF(\"ocean\", Doc3) = 1/7 ≈ 0.1429\n",
    "\n",
    "**2. Inverse Document Frequency (IDF) of \"ocean\"**:\n",
    "\n",
    "\"ocean\" appears only in `Doc3`:\n",
    "\n",
    "IDF(\"ocean\") = log(3/1) = log(3) ≈ 1.0986\n",
    "\n",
    "**3. Compute TF-IDF**:\n",
    "\n",
    "TF-IDF(\"ocean\", Doc3) = TF(\"ocean\", Doc3) * IDF(\"ocean\")\n",
    "                      ≈ 0.1429 * 1.0986\n",
    "                      ≈ 0.157\n",
    "\n",
    "Here, since \"ocean\" is unique to `Doc3`, it gets a higher TF-IDF value, indicating its importance or specificity to that document.\n",
    "\n",
    "This example showcases how TF-IDF helps in weighing down common words and giving importance to words that are more unique to specific documents in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['Email']).toarray()\n",
    "\n",
    "# Split the data again using the TF-IDF representation\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier using TF-IDF representation\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with TF-IDF: 0.74\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.82      0.68      0.74       410\n",
      "         Yes       0.66      0.81      0.73       322\n",
      "\n",
      "    accuracy                           0.74       732\n",
      "   macro avg       0.74      0.74      0.74       732\n",
      "weighted avg       0.75      0.74      0.74       732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predict on the test set\n",
    "y_pred_tfidf = clf.predict(X_test_tfidf)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "print(f\"Accuracy with TF-IDF: {accuracy_tfidf:.2f}\")\n",
    "\n",
    "# Display a more detailed classification report\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the model trained on the TF-IDF representation has improved slightly in accuracy compared to the earlier Bag of Words (BoW) representation. The accuracy is now 0.74 or 74%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-grams and Tri-grams Can Improve a Model\n",
    "\n",
    "Why Adding Bi-grams and Tri-grams Can Improve a Model\n",
    "When we process textual data for machine learning, we often convert the text into numerical vectors that the model can understand. One of the simplest ways to do this is using unigrams, which means every word is treated as a separate feature. For instance, in the sentence \"I love ice cream\", the unigrams would be [\"I\", \"love\", \"ice\", \"cream\"].\n",
    "\n",
    "However, sometimes just looking at individual words might not be enough. Words in combination can carry very different meanings than when they stand alone.\n",
    "\n",
    "Bi-grams are combinations of two words. For our example sentence, the bi-grams would be [\"I love\", \"love ice\", \"ice cream\"].\n",
    "Tri-grams are combinations of three words. For the same sentence, the tri-grams would be [\"I love ice\", \"love ice cream\"].\n",
    "By looking at bi-grams and tri-grams, we can start to understand phrases and combinations of words that occur frequently together and might have a specific meaning in our dataset.\n",
    "\n",
    "However, a word of caution: While adding bi-grams and tri-grams can improve the model by providing more context and capturing phrases, it also increases the number of features. This can sometimes lead to overfitting, where the model performs well on the training data but poorly on unseen data. So, it's essential to ensure that the model is still generalizing well to new, unseen data after adding these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# I'm assuming you've defined data['Email'] and data['Label'] somewhere earlier in your code\n",
    "\n",
    "# Initialize the TF-IDF vectorizer to consider both unigrams and bi-grams\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "\n",
    "# Convert the emails to a matrix of TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['Email']).toarray()\n",
    "\n",
    "# Split the data using the TF-IDF representation\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the classifier using TF-IDF representation\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00 am', '00 pm', '000 to', '02 28', '10 00', '10 000', '10 am', '11 00', '12 17', '12 18', '15 minutes', '15 off', '18 11', '20 2001', '20 off', '28 12', '30 am', '30 days', '713 853', 'ability to']\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (terms) from the TF-IDF vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Filter out only the bi-grams\n",
    "bigrams = [name for name in feature_names if len(name.split()) == 2]\n",
    "\n",
    "# Display first few bi-grams for inspection\n",
    "print(bigrams[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you: 51.14051089687338\n",
      "want to: 33.49116748708142\n",
      "like to: 30.073947712635388\n",
      "of the: 24.72515390265713\n",
      "in the: 24.54936529341248\n",
      "you want: 23.836663352338345\n",
      "for the: 23.099790245531487\n",
      "on the: 22.925782535739252\n",
      "to the: 22.60780698370452\n",
      "to get: 22.431956076413766\n",
      "to discuss: 22.105021834806223\n",
      "we can: 22.016163729913757\n",
      "you have: 19.454156518984995\n",
      "would like: 19.137833602897544\n",
      "let me: 17.977884373988957\n",
      "can you: 17.753254490125038\n",
      "you can: 17.23751258012659\n",
      "need to: 16.93172787579045\n",
      "will be: 16.442565945185805\n",
      "me know: 15.607661079727693\n"
     ]
    }
   ],
   "source": [
    "# Sum the occurrences of each bigram across all documents\n",
    "bigram_counts = X_tfidf[:, [tfidf_vectorizer.vocabulary_.get(bigram) for bigram in bigrams]].sum(axis=0)\n",
    "\n",
    "# Pair the bigrams with their counts and sort by count\n",
    "sorted_bigrams = sorted(list(zip(bigrams, bigram_counts)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top most common bigrams for inspection\n",
    "for bigram, count in sorted_bigrams[:20]:\n",
    "    print(f\"{bigram}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you want to: 20.640112661442704\n",
      "would like to: 17.348516265611778\n",
      "let me know: 15.484621040662189\n",
      "if you want: 12.659722005554581\n",
      "if you have: 12.643920456016206\n",
      "you like to: 10.587303120841366\n",
      "you have any: 9.927880269075\n",
      "if you like: 8.53681926087871\n",
      "you would like: 8.52439127702905\n",
      "if you would: 8.435820434453408\n",
      "have any questions: 7.58148426006524\n",
      "look forward to: 7.549154400095598\n",
      "give me call: 7.044861300865557\n",
      "be able to: 6.559982639521789\n",
      "to discuss the: 6.174010709944985\n",
      "we need to: 5.6873932622681895\n",
      "when you get: 5.648832412136823\n",
      "to make sure: 5.5420886862673555\n",
      "me know if: 5.474501100233474\n",
      "you need to: 5.383895865729841\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (terms) from the TF-IDF vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Filter out only the trigrams\n",
    "trigrams = [name for name in feature_names if len(name.split()) == 3]\n",
    "\n",
    "# Sum the occurrences of each trigram across all documents\n",
    "trigram_counts = X_tfidf[:, [tfidf_vectorizer.vocabulary_.get(trigram) for trigram in trigrams]].sum(axis=0)\n",
    "\n",
    "# Pair the trigrams with their counts and sort by count\n",
    "sorted_trigrams = sorted(list(zip(trigrams, trigram_counts)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top most common trigrams for inspection\n",
    "for trigram, count in sorted_trigrams[:20]:\n",
    "    print(f\"{trigram}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with TF-IDF: 0.79\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.82      0.80      0.81       410\n",
      "         Yes       0.75      0.78      0.76       322\n",
      "\n",
      "    accuracy                           0.79       732\n",
      "   macro avg       0.79      0.79      0.79       732\n",
      "weighted avg       0.79      0.79      0.79       732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predict on the test set\n",
    "y_pred_tfidf = clf.predict(X_test_tfidf)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "print(f\"Accuracy with TF-IDF: {accuracy_tfidf:.2f}\")\n",
    "\n",
    "# Display a more detailed classification report\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Submitted_ CSCI Lab Wall Adhesive on Design Work Request.eml: No\n",
      "[Starfish] The Struggle to Keep Pace item you created for Felix Lidenmark has been closed.eml: No\n",
      "McGraw Hill GO provides students with guardrails for your American History Survey Texts course.eml: No\n",
      "On Israel, Hamas, and Gaza.eml: No\n",
      "ENGR-CSCI-MATH-498-x1-META.U2.2023FA_ Re_ Week 7 3.eml: No\n",
      "ENGR-CSCI-MATH-498-x1-META.U2.2023FA_ Re_ Week 7 2.eml: No\n",
      "RE_ Ethics and AI Grant.eml: No\n",
      "RE_ Ethics and AI Grant_ Krister Mattson 2.eml: No\n",
      "RE_ Ethics and AI Grant_ Krister Mattson 3.eml: No\n",
      "Request your digital copy of Chris Suh's The Allure of Empire today.eml: No\n",
      "ENGR-CSCI-MATH-498-x1-META.U2.2023FA_ Re_ Week 7 5.eml: No\n",
      "ENGR-CSCI-MATH-498-x1-META.U2.2023FA_ Re_ Week 7 4.eml: No\n",
      "Microsoft patches are complete..eml: No\n",
      "RE_ Ethics and AI Grant_ Krister Mattson.eml: No\n",
      "Ethics and AI Grant_ Krister Mattson.eml: No\n",
      "Overdue Notification.eml: No\n",
      "RE_ Ethics and AI Grant_ Krister Mattson 4.eml: No\n",
      "RE_ Ethics and AI Grant_ Krister Mattson 5.eml: No\n",
      "Hypothesis_ 2 annotations from 1 student in 1 course.eml: No\n",
      "Microsoft patching for the Colleague servers tonight at 5_00pm.eml: Yes\n",
      "How Shutterstock boosted KPIs and lowered costs.eml: No\n",
      "Little-Known Secret To Boost Your Growth Metrics With Customer Identity🚀.eml: No\n",
      "Last-minute openings Monday!.eml: No\n",
      "Marketing AI Institute Keynote at Engage Boston!.eml: No\n",
      "A Little Something to Say Thank You.eml: No\n",
      "Curl Fixes.eml: No\n",
      "ENGR-CSCI-MATH-498-x1-META.U2.2023FA_ Re_ Week 7.eml: No\n",
      "AI in the classroom—Discover challenges and opportunities.eml: No\n",
      "Celebrating Hispanic and Latin American Voices.eml: No\n",
      "Request Summary.eml: No\n",
      "Immigration Documentaries to Inspire Action.eml: No\n",
      "Digital Innovation Trailblazing_ CIO lessons from leading brands.eml: No\n",
      "egweinberg, you're invited to AU 2023.eml: No\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import email\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_content_from_eml(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        msg = BytesParser(policy=policy.default).parse(f)\n",
    "    content = ''\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                charset = part.get_content_charset()\n",
    "                content += part.get_payload(decode=True).decode(charset if charset else 'utf-8', 'ignore')\n",
    "            elif part.get_content_type() == 'text/html':\n",
    "                charset = part.get_content_charset()\n",
    "                html_content = part.get_payload(decode=True).decode(charset if charset else 'utf-8', 'ignore')\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                content += soup.get_text(separator=' ')\n",
    "    else:\n",
    "        charset = msg.get_content_charset()\n",
    "        content_type = msg.get_content_type()\n",
    "        raw_content = msg.get_payload(decode=True).decode(charset if charset else 'utf-8', 'ignore')\n",
    "        if content_type == 'text/html':\n",
    "            soup = BeautifulSoup(raw_content, 'html.parser')\n",
    "            content = soup.get_text(separator=' ')\n",
    "        else:\n",
    "            content = raw_content\n",
    "    \n",
    "    return content.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup \n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "def preprocess(email_content):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(email_content)\n",
    "    \n",
    "    # Removing stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Convert list of tokens back to string\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    # BoW Vectorization (If needed)\n",
    "    # vectorized_content = vectorizer.transform([preprocessed_text]).toarray()\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Set directory path\n",
    "directory_path = 'Mail'\n",
    "\n",
    "# Loop through each .eml file in the directory\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.eml'):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # Extract content\n",
    "        content = extract_content_from_eml(file_path)\n",
    "        #print(content)\n",
    "        # Skip emails with empty content\n",
    "        if not content.strip():\n",
    "            print(f\"Skipping {file_name} due to empty content.\")\n",
    "            continue\n",
    "        \n",
    "        # Preprocess the content\n",
    "        processed_content = preprocess(content)\n",
    "        \n",
    "        # Vectorize the content\n",
    "        vectorized_content = tfidf_vectorizer.transform([processed_content]).toarray()\n",
    "        \n",
    "        # Predict the label\n",
    "        predicted_label = clf.predict(vectorized_content)[0]\n",
    "        \n",
    "        # Print the email file name and its predicted label\n",
    "        print(f\"{file_name}: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
